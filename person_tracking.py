#!/usr/bin/env python3
"""
RK3576 YOLOv8 äººä½“è·Ÿè¸ª + MJPEG æ¨æµ
ä¸“ä¸ºç”µæœºè·Ÿè¸ªè®¾è®¡ï¼ŒåŒ…å«ï¼š
- å¯é…ç½®æ£€æµ‹é¢‘ç‡
- ä½ç½®å¹³æ»‘æ»¤æ³¢
- ç›®æ ‡æŒç»­è·Ÿè¸ª
- è¾“å‡ºç”µæœºæ§åˆ¶åæ ‡
"""

import cv2
import numpy as np
import argparse
import threading
import time
from io import BytesIO
from collections import deque
from rknnlite.api import RKNNLite
from http.server import HTTPServer, BaseHTTPRequestHandler
import socket
import json

# ============== å‚æ•°è®¾ç½® ==============
OBJ_THRESH = 0.25
NMS_THRESH = 0.45
IMG_SIZE = (640, 640)

# COCO 80ç±»ï¼ˆåªéœ€è¦ personï¼‰
CLASSES = ("person", "bicycle", "car", "motorbike", "aeroplane", "bus", "train", "truck", "boat", "traffic light",
           "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat", "dog", "horse", "sheep", "cow", "elephant",
           "bear", "zebra", "giraffe", "backpack", "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball", "kite",
           "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket", "bottle", "wine glass", "cup", "fork", "knife",
           "spoon", "bowl", "banana", "apple", "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake", "chair", "sofa",
           "pottedplant", "bed", "diningtable", "toilet", "tvmonitor", "laptop", "mouse", "remote", "keyboard", "cell phone", "microwave",
           "oven", "toaster", "sink", "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush")


# ============== åå¤„ç†å‡½æ•° ===============
def dfl(position):
    n, c, h, w = position.shape
    p_num = 4
    mc = c // p_num
    position = position.reshape(n, p_num, mc, h, w)
    position = np.exp(position) / (np.exp(position).sum(axis=2, keepdims=True) + 1e-6)
    acc_metrix = np.arange(mc, dtype=np.float32).reshape(1, 1, mc, 1, 1)
    position = (position * acc_metrix).sum(axis=2)
    return position


def box_process(position):
    grid_h, grid_w = position.shape[2:4]
    col, row = np.meshgrid(np.arange(0, grid_w), np.arange(0, grid_h))
    col = col.reshape(1, 1, grid_h, grid_w)
    row = row.reshape(1, 1, grid_h, grid_w)
    grid = np.concatenate((col, row), axis=1)
    stride = np.array([IMG_SIZE[1] // grid_h, IMG_SIZE[0] // grid_w]).reshape(1, 2, 1, 1)
    position = dfl(position)
    box_xy = grid + 0.5 - position[:, 0:2, :, :]
    box_xy2 = grid + 0.5 + position[:, 2:4, :, :]
    xyxy = np.concatenate((box_xy * stride, box_xy2 * stride), axis=1)
    return xyxy


def filter_boxes(boxes, box_confidences, box_class_probs):
    box_confidences = box_confidences.reshape(-1)
    class_max_score = np.max(box_class_probs, axis=-1)
    classes = np.argmax(box_class_probs, axis=-1)
    _class_pos = np.where(class_max_score * box_confidences >= OBJ_THRESH)
    scores = (class_max_score * box_confidences)[_class_pos]
    boxes = boxes[_class_pos]
    classes = classes[_class_pos]
    return boxes, classes, scores


def nms_boxes(boxes, scores):
    x = boxes[:, 0]
    y = boxes[:, 1]
    w = boxes[:, 2] - boxes[:, 0]
    h = boxes[:, 3] - boxes[:, 1]
    areas = w * h
    order = scores.argsort()[::-1]
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x[i], x[order[1:]])
        yy1 = np.maximum(y[i], y[order[1:]])
        xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])
        yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])
        w1 = np.maximum(0.0, xx2 - xx1 + 0.00001)
        h1 = np.maximum(0.0, yy2 - yy1 + 0.00001)
        inter = w1 * h1
        ovr = inter / (areas[i] + areas[order[1:]] - inter)
        inds = np.where(ovr <= NMS_THRESH)[0]
        order = order[inds + 1]
    keep = np.array(keep)
    return keep


def post_process(input_data):
    boxes, scores, classes_conf = [], [], []
    default_branch = 3
    pair_per_branch = len(input_data) // default_branch

    for i in range(default_branch):
        boxes.append(box_process(input_data[pair_per_branch * i]))
        classes_conf.append(input_data[pair_per_branch * i + 1])
        scores.append(np.ones_like(input_data[pair_per_branch * i + 1][:, :1, :, :], dtype=np.float32))

    def sp_flatten(_in):
        ch = _in.shape[1]
        _in = _in.transpose(0, 2, 3, 1)
        return _in.reshape(-1, ch)

    boxes = [sp_flatten(_v) for _v in boxes]
    classes_conf = [sp_flatten(_v) for _v in classes_conf]
    scores = [sp_flatten(_v) for _v in scores]

    boxes = np.concatenate(boxes)
    classes_conf = np.concatenate(classes_conf)
    scores = np.concatenate(scores)

    boxes, classes, scores = filter_boxes(boxes, scores, classes_conf)

    nboxes, nclasses, nscores = [], [], []
    for c in set(classes):
        inds = np.where(classes == c)
        b = boxes[inds]
        c = classes[inds]
        s = scores[inds]
        keep = nms_boxes(b, s)
        if len(keep) != 0:
            nboxes.append(b[keep])
            nclasses.append(c[keep])
            nscores.append(s[keep])

    if not nclasses and not nscores:
        return None, None, None

    boxes = np.concatenate(nboxes)
    classes = np.concatenate(nclasses)
    scores = np.concatenate(nscores)
    return boxes, classes, scores


class YOLOv8Detector:
    def __init__(self, rknn_model='yolov8.rknn', target_size=640):
        self.target_size = target_size
        self.img_size = (target_size, target_size)

        print(f'--> Loading RKNN model: {rknn_model}')
        self.rknn_lite = RKNNLite()
        ret = self.rknn_lite.load_rknn(rknn_model)
        if ret != 0:
            raise RuntimeError('Load RKNN model failed!')

        print('--> Init RKNN runtime')
        ret = self.rknn_lite.init_runtime()
        if ret != 0:
            raise RuntimeError('Init runtime failed!')

        print('âœ… Model loaded successfully')

    def letterbox(self, img):
        img_h, img_w = img.shape[:2]
        shape = (img_h, img_w)
        scale = min(self.img_size[0] / shape[0], self.img_size[1] / shape[1])
        new_unpad = (int(round(shape[1] * scale)), int(round(shape[0] * scale)))
        dw_total, dh_total = self.img_size[1] - new_unpad[0], self.img_size[0] - new_unpad[1]
        dw = dw_total / 2
        dh = dh_total / 2

        if shape[::-1] != new_unpad:
            resized = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
        else:
            resized = img

        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
        img_padded = cv2.copyMakeBorder(resized, top, bottom, left, right,
                                        cv2.BORDER_CONSTANT, value=(0, 0, 0))

        img_rgb = cv2.cvtColor(img_padded, cv2.COLOR_BGR2RGB)
        input_data = np.expand_dims(img_rgb, axis=0)

        return input_data, scale, dw, dh

    def detect_frame(self, frame):
        img_h, img_w = frame.shape[:2]

        # é¢„å¤„ç†
        input_data, scale, dw, dh = self.letterbox(frame)

        # æ¨ç†
        try:
            outputs = self.rknn_lite.inference(inputs=[input_data])
        except KeyboardInterrupt:
            raise
        except Exception as e:
            return None, None, None

        # åå¤„ç†
        boxes, classes, scores = post_process(outputs)

        if boxes is None:
            return None, None, None

        # åæ ‡è½¬æ¢
        boxes[:, 0] = (boxes[:, 0] - dw) / scale
        boxes[:, 1] = (boxes[:, 1] - dh) / scale
        boxes[:, 2] = (boxes[:, 2] - dw) / scale
        boxes[:, 3] = (boxes[:, 3] - dh) / scale

        # é™åˆ¶åœ¨å›¾åƒèŒƒå›´å†…
        boxes[:, 0] = np.clip(boxes[:, 0], 0, img_w)
        boxes[:, 1] = np.clip(boxes[:, 1], 0, img_h)
        boxes[:, 2] = np.clip(boxes[:, 2], 0, img_w)
        boxes[:, 3] = np.clip(boxes[:, 3], 0, img_h)

        return boxes, classes, scores

    def release(self):
        if hasattr(self, 'rknn_lite'):
            self.rknn_lite.release()


class TrackedTarget:
    """è¢«è·Ÿè¸ªçš„ç›®æ ‡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    def __init__(self, track_id, box, score, img_width, img_height):
        self.id = track_id
        self.score = score
        self.first_seen = time.time()  # é¦–æ¬¡æ£€æµ‹æ—¶é—´
        self.last_seen = time.time()   # æœ€åæ£€æµ‹æ—¶é—´
        self.frame_count = 1           # è·Ÿè¸ªå¸§æ•°

        # è¾¹ç•Œæ¡† (x1, y1, x2, y2)
        self.box = box
        self.box_width = box[2] - box[0]
        self.box_height = box[3] - box[1]
        self.box_area = self.box_width * self.box_height

        # ä¸­å¿ƒç‚¹
        self.center_x = (box[0] + box[2]) / 2
        self.center_y = (box[1] + box[3]) / 2

        # å¹³æ»‘æ»¤æ³¢ï¼ˆä½¿ç”¨ç§»åŠ¨å¹³å‡ï¼‰
        self.history_x = deque(maxlen=5)
        self.history_y = deque(maxlen=5)
        self.history_x.append(self.center_x)
        self.history_y.append(self.center_y)

        # å¹³æ»‘åçš„ä¸­å¿ƒç‚¹
        self.smooth_x = self.center_x
        self.smooth_y = self.center_y

        # ç›¸å¯¹ä½ç½®ï¼ˆ-1 åˆ° 1ï¼Œç”¨äºç”µæœºæ§åˆ¶ï¼‰
        self.rel_x = (self.center_x - img_width / 2) / (img_width / 2)
        self.rel_y = (self.center_y - img_height / 2) / (img_height / 2)

        # è·ç¦»å›¾åƒä¸­å¿ƒçš„åƒç´ è·ç¦»
        self.dist_x = self.center_x - img_width / 2
        self.dist_y = self.center_y - img_height / 2
        self.distance = np.sqrt(self.dist_x ** 2 + self.dist_y ** 2)

        # é€Ÿåº¦ï¼ˆåƒç´ /ç§’ï¼‰
        self.vx = 0.0
        self.vy = 0.0
        self.speed = 0.0

        # è¿åŠ¨æ–¹å‘ï¼ˆè§’åº¦ï¼Œ0-360åº¦ï¼‰
        self.angle = 0.0

        # ä¸¢å¤±è®¡æ•°ï¼ˆç”¨äºåˆ¤æ–­ç›®æ ‡æ˜¯å¦æ¶ˆå¤±ï¼‰
        self.lost_count = 0

        # è¿åŠ¨çŠ¶æ€
        self.is_moving = False
        self.direction_str = "é™æ­¢"

    def update(self, box, score, img_width, img_height):
        """æ›´æ–°ç›®æ ‡ä½ç½®"""
        current_time = time.time()

        # è®°å½•ä¸Šä¸€å¸§ä½ç½®
        prev_center_x = self.smooth_x
        prev_center_y = self.smooth_y
        prev_time = self.last_seen

        # æ›´æ–°åŸºæœ¬ä¿¡æ¯
        self.box = box
        self.score = score
        self.box_width = box[2] - box[0]
        self.box_height = box[3] - box[1]
        self.box_area = self.box_width * self.box_height

        # æ–°çš„ä¸­å¿ƒç‚¹
        new_center_x = (box[0] + box[2]) / 2
        new_center_y = (box[1] + box[3]) / 2

        # æ·»åŠ åˆ°å†å²è®°å½•
        self.history_x.append(new_center_x)
        self.history_y.append(new_center_y)

        # è®¡ç®—å¹³æ»‘åçš„ä½ç½®ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
        self.smooth_x = sum(self.history_x) / len(self.history_x)
        self.smooth_y = sum(self.history_y) / len(self.history_y)

        # ç›¸å¯¹ä½ç½®
        self.rel_x = (self.smooth_x - img_width / 2) / (img_width / 2)
        self.rel_y = (self.smooth_y - img_height / 2) / (img_height / 2)

        # è·ç¦»å›¾åƒä¸­å¿ƒ
        self.dist_x = self.smooth_x - img_width / 2
        self.dist_y = self.smooth_y - img_height / 2
        self.distance = np.sqrt(self.dist_x ** 2 + self.dist_y ** 2)

        # è®¡ç®—é€Ÿåº¦å’Œæ—¶é—´å·®
        dt = current_time - prev_time
        if dt > 0:
            dx = self.smooth_x - prev_center_x
            dy = self.smooth_y - prev_center_y
            self.vx = dx / dt  # åƒç´ /ç§’
            self.vy = dy / dt
            self.speed = np.sqrt(self.vx ** 2 + self.vy ** 2)

            # è®¡ç®—è¿åŠ¨æ–¹å‘ï¼ˆè§’åº¦ï¼‰
            if self.speed > 5:  # é€Ÿåº¦å¤§äº5åƒç´ /ç§’æ‰è®¤ä¸ºåœ¨è¿åŠ¨
                self.angle = np.arctan2(-dy, dx) * 180 / np.pi  # -dyå› ä¸ºyè½´å‘ä¸‹
                if self.angle < 0:
                    self.angle += 360
                self.is_moving = True

                # æ–¹å‘æè¿°
                if 337.5 <= self.angle or self.angle < 22.5:
                    self.direction_str = "å‘å³"
                elif 22.5 <= self.angle < 67.5:
                    self.direction_str = "å³ä¸‹"
                elif 67.5 <= self.angle < 112.5:
                    self.direction_str = "å‘ä¸‹"
                elif 112.5 <= self.angle < 157.5:
                    self.direction_str = "å·¦ä¸‹"
                elif 157.5 <= self.angle < 202.5:
                    self.direction_str = "å‘å·¦"
                elif 202.5 <= self.angle < 247.5:
                    self.direction_str = "å·¦ä¸Š"
                elif 247.5 <= self.angle < 292.5:
                    self.direction_str = "å‘ä¸Š"
                elif 292.5 <= self.angle < 337.5:
                    self.direction_str = "å³ä¸Š"
            else:
                self.is_moving = False
                self.direction_str = "é™æ­¢"
        else:
            self.vx = 0.0
            self.vy = 0.0
            self.speed = 0.0
            self.is_moving = False
            self.direction_str = "é™æ­¢"

        # æ›´æ–°æ—¶é—´
        self.last_seen = current_time
        self.frame_count += 1

        # é‡ç½®ä¸¢å¤±è®¡æ•°
        self.lost_count = 0

    def get_smooth_center(self):
        """è·å–å¹³æ»‘åçš„ä¸­å¿ƒç‚¹"""
        return self.smooth_x, self.smooth_y

    def get_relative_position(self):
        """è·å–ç›¸å¯¹ä½ç½®ï¼ˆ-1 åˆ° 1ï¼‰"""
        return self.rel_x, self.rel_y

    def get_velocity(self):
        """è·å–é€Ÿåº¦ï¼ˆvx, vy, speedï¼‰"""
        return self.vx, self.vy, self.speed

    def to_dict(self, img_width, img_height):
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äº JSON è¾“å‡ºï¼‰"""
        return {
            'id': self.id,
            'confidence': float(self.score),
            'tracking': {
                'frames': self.frame_count,
                'duration': float(self.last_seen - self.first_seen),
                'lost_count': self.lost_count
            },
            'position': {
                'center': {'x': float(self.smooth_x), 'y': float(self.smooth_y)},
                'bbox': {
                    'x1': float(self.box[0]),
                    'y1': float(self.box[1]),
                    'x2': float(self.box[2]),
                    'y2': float(self.box[3]),
                    'width': float(self.box_width),
                    'height': float(self.box_height),
                    'area': float(self.box_area)
                },
                'relative': {  # -1 åˆ° 1ï¼Œé€‚åˆç”µæœºæ§åˆ¶
                    'x': float(self.rel_x),
                    'y': float(self.rel_y)
                },
                'distance_from_center': {
                    'x': float(self.dist_x),  # åƒç´ 
                    'y': float(self.dist_y),  # åƒç´ 
                    'euclidean': float(self.distance)  # åƒç´ 
                }
            },
            'motion': {
                'is_moving': self.is_moving,
                'velocity': {
                    'vx': float(self.vx),  # åƒç´ /ç§’ï¼Œå‘å³ä¸ºæ­£
                    'vy': float(self.vy),  # åƒç´ /ç§’ï¼Œå‘ä¸‹ä¸ºæ­£
                    'speed': float(self.speed)  # åƒç´ /ç§’
                },
                'direction': {
                    'angle': float(self.angle),  # åº¦æ•°ï¼Œ0=å³ï¼Œ90=ä¸Šï¼Œ180=å·¦ï¼Œ270=ä¸‹
                    'text': self.direction_str
                }
            },
            'timestamp': float(self.last_seen)
        }


class PersonTracker:
    """äººä½“è·Ÿè¸ªå™¨"""
    def __init__(self, max_distance=100, iou_threshold=0.3):
        self.max_distance = max_distance  # æœ€å¤§è·ç¦»ï¼ˆåƒç´ ï¼‰
        self.iou_threshold = iou_threshold  # IOU é˜ˆå€¼
        self.next_id = 1
        self.targets = {}  # {track_id: TrackedTarget}
        self.img_width = 1280
        self.img_height = 720

    def set_image_size(self, width, height):
        """è®¾ç½®å›¾åƒå°ºå¯¸"""
        self.img_width = width
        self.img_height = height

    def calculate_iou(self, box1, box2):
        """è®¡ç®— IOU"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])

        if x2 <= x1 or y2 <= y1:
            return 0.0

        intersection = (x2 - x1) * (y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection

        return intersection / union if union > 0 else 0.0

    def calculate_distance(self, box1, box2):
        """è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦»"""
        center1_x = (box1[0] + box1[2]) / 2
        center1_y = (box1[1] + box1[3]) / 2
        center2_x = (box2[0] + box2[2]) / 2
        center2_y = (box2[1] + box2[3]) / 2

        return np.sqrt((center1_x - center2_x) ** 2 + (center1_y - center2_y) ** 2)

    def update(self, boxes, classes, scores):
        """æ›´æ–°è·Ÿè¸ªç›®æ ‡"""
        # å¤„ç†æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡çš„æƒ…å†µ
        if boxes is None or classes is None or scores is None:
            # æ²¡æœ‰æ£€æµ‹åˆ°ç›®æ ‡ï¼Œå¢åŠ æ‰€æœ‰ç›®æ ‡çš„ä¸¢å¤±è®¡æ•°
            for track_id in list(self.targets.keys()):
                self.targets[track_id].lost_count += 1
                # å¦‚æœä¸¢å¤±å¤ªä¹…ï¼Œåˆ é™¤ç›®æ ‡
                if self.targets[track_id].lost_count > 10:  # ä¸¢å¤±10å¸§ååˆ é™¤
                    del self.targets[track_id]
            return

        # åªè·Ÿè¸ª person (class_id = 0)
        person_indices = [i for i, cls in enumerate(classes) if cls == 0]
        person_boxes = boxes[person_indices] if len(person_indices) > 0 else None

        if person_boxes is None or len(person_boxes) == 0:
            # æ²¡æœ‰æ£€æµ‹åˆ°äººï¼Œå¢åŠ æ‰€æœ‰ç›®æ ‡çš„ä¸¢å¤±è®¡æ•°
            for track_id in list(self.targets.keys()):
                self.targets[track_id].lost_count += 1
                # å¦‚æœä¸¢å¤±å¤ªä¹…ï¼Œåˆ é™¤ç›®æ ‡
                if self.targets[track_id].lost_count > 10:  # ä¸¢å¤±10å¸§ååˆ é™¤
                    del self.targets[track_id]
            return

        # ä¸ºæ¯ä¸ªæ£€æµ‹æ¡†æ‰¾åˆ°åŒ¹é…çš„è·Ÿè¸ªç›®æ ‡
        matched_tracks = set()
        matched_detections = set()

        # å…ˆå°è¯•ç”¨ IOU åŒ¹é…
        for i, box in enumerate(person_boxes):
            best_iou = 0
            best_track_id = None

            for track_id, target in self.targets.items():
                if track_id in matched_tracks:
                    continue

                iou = self.calculate_iou(target.box, box)
                if iou > best_iou and iou > self.iou_threshold:
                    best_iou = iou
                    best_track_id = track_id

            if best_track_id is not None:
                # æ‰¾åˆ°åŒ¹é…ï¼Œæ›´æ–°ç›®æ ‡
                self.targets[best_track_id].update(box, scores[person_indices[i]], self.img_width, self.img_height)
                matched_tracks.add(best_track_id)
                matched_detections.add(i)
            else:
                # æ²¡æ‰¾åˆ°åŒ¹é…ï¼Œå°è¯•ç”¨è·ç¦»åŒ¹é…
                best_dist = float('inf')
                best_track_id = None

                for track_id, target in self.targets.items():
                    if track_id in matched_tracks:
                        continue

                    dist = self.calculate_distance(target.box, box)
                    if dist < best_dist and dist < self.max_distance:
                        best_dist = dist
                        best_track_id = track_id

                if best_track_id is not None:
                    self.targets[best_track_id].update(box, scores[person_indices[i]], self.img_width, self.img_height)
                    matched_tracks.add(best_track_id)
                    matched_detections.add(i)
                else:
                    # åˆ›å»ºæ–°ç›®æ ‡
                    new_target = TrackedTarget(self.next_id, box, scores[person_indices[i]], self.img_width, self.img_height)
                    self.targets[self.next_id] = new_target
                    matched_tracks.add(self.next_id)
                    matched_detections.add(i)
                    self.next_id += 1

        # å¤„ç†æœªåŒ¹é…çš„æ£€æµ‹æ¡†ï¼ˆæ–°ç›®æ ‡ï¼‰
        for i in range(len(person_boxes)):
            if i not in matched_detections:
                new_target = TrackedTarget(self.next_id, person_boxes[i], scores[person_indices[i]], self.img_width, self.img_height)
                self.targets[self.next_id] = new_target
                self.next_id += 1

        # å¤„ç†æœªåŒ¹é…çš„è·Ÿè¸ªç›®æ ‡ï¼ˆç›®æ ‡æ¶ˆå¤±ï¼‰
        for track_id in list(self.targets.keys()):
            if track_id not in matched_tracks:
                self.targets[track_id].lost_count += 1
                if self.targets[track_id].lost_count > 10:
                    del self.targets[track_id]

    def get_primary_target(self):
        """è·å–ä¸»è¦ç›®æ ‡ï¼ˆç½®ä¿¡åº¦æœ€é«˜çš„ï¼‰"""
        if not self.targets:
            return None

        # è¿”å›ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡
        primary_target = max(self.targets.values(), key=lambda t: t.score)
        return primary_target

    def get_all_targets(self):
        """è·å–æ‰€æœ‰ç›®æ ‡"""
        return list(self.targets.values())


def draw_tracking(frame, tracker, show_all=True):
    """ç»˜åˆ¶è·Ÿè¸ªç»“æœ"""
    if show_all:
        # æ˜¾ç¤ºæ‰€æœ‰è¢«è·Ÿè¸ªçš„ç›®æ ‡
        for target in tracker.get_all_targets():
            x1, y1, x2, y2 = map(int, target.box)

            # é¢œè‰²æ ¹æ® ID å˜åŒ–
            color = (
                int((target.id * 50) % 255),
                int((target.id * 100) % 255),
                int((target.id * 150) % 255)
            )

            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)

            # ç»˜åˆ¶ ID å’Œç½®ä¿¡åº¦
            label = f'ID:{target.id} {target.score:.2f}'
            cv2.putText(frame, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

            # ç»˜åˆ¶å¹³æ»‘åçš„ä¸­å¿ƒç‚¹
            smooth_x, smooth_y = target.get_smooth_center()
            cv2.circle(frame, (int(smooth_x), int(smooth_y)), 5, color, -1)

            # ç»˜åˆ¶ç›¸å¯¹ä½ç½®
            rel_x, rel_y = target.get_relative_position()
            info = f'({rel_x:.2f}, {rel_y:.2f})'
            cv2.putText(frame, info, (x1, y2 + 15),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

    else:
        # åªæ˜¾ç¤ºä¸»è¦ç›®æ ‡
        primary = tracker.get_primary_target()
        if primary:
            x1, y1, x2, y2 = map(int, primary.box)

            # ä¸»ç›®æ ‡ç”¨ç»¿è‰²
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)

            # ç»˜åˆ¶å¹³æ»‘ä¸­å¿ƒç‚¹
            smooth_x, smooth_y = primary.get_smooth_center()
            cv2.circle(frame, (int(smooth_x), int(smooth_y)), 8, (0, 255, 0), -1)
            cv2.circle(frame, (int(smooth_x), int(smooth_y)), 15, (0, 255, 0), 2)

            # æ˜¾ç¤ºä¿¡æ¯
            label = f'Target {primary.id}: {primary.score:.2f}'
            cv2.putText(frame, label, (x1, y1 - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            # æ˜¾ç¤ºç›¸å¯¹åæ ‡
            rel_x, rel_y = primary.get_relative_position()
            coord_info = f'Pos: ({rel_x:.2f}, {rel_y:.2f})'
            cv2.putText(frame, coord_info, (x1, y2 + 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            # ç»˜åˆ¶åå­—çº¿
            h, w = frame.shape[:2]
            cv2.line(frame, (int(smooth_x), 0), (int(smooth_x), h), (0, 255, 0), 1)
            cv2.line(frame, (0, int(smooth_y)), (w, int(smooth_y)), (0, 255, 0), 1)

    return frame


class MJPEGStreamer:
    """MJPEG æµæœåŠ¡å™¨"""
    def __init__(self, port=8080):
        self.port = port
        self.frame = None
        self.running = False
        self.fps = 30  # æ¨æµå¸§ç‡

    def start(self):
        self.running = True

        class StreamHandler(BaseHTTPRequestHandler):
            def __init__(self, *args, streamer=None, **kwargs):
                self.streamer = streamer
                super().__init__(*args, **kwargs)

            def do_GET(self):
                if self.path == '/' or self.path == '/stream':
                    self.send_response(200)
                    self.send_header('Content-type', 'multipart/x-mixed-replace; boundary=--frame')
                    self.end_headers()

                    try:
                        while self.streamer.running:
                            if self.streamer.frame is not None:
                                ret, buffer = cv2.imencode('.jpg', self.streamer.frame, [cv2.IMWRITE_JPEG_QUALITY, 80])
                                frame_bytes = buffer.tobytes()

                                self.wfile.write(b'--frame\r\n')
                                self.wfile.write(b'Content-Type: image/jpeg\r\n\r\n')
                                self.wfile.write(frame_bytes)
                                self.wfile.write(b'\r\n\r\n')

                                time.sleep(1.0 / self.streamer.fps)
                    except (BrokenPipeError, ConnectionResetError):
                        pass
                else:
                    self.send_error(404)

            def log_message(self, format, *args):
                pass  # ç¦ç”¨æ—¥å¿—

        def handler(*args, **kwargs):
            StreamHandler(*args, streamer=self, **kwargs)

        server = HTTPServer(('0.0.0.0', self.port), handler)
        thread = threading.Thread(target=server.serve_forever, daemon=True)
        thread.start()

    def update_frame(self, frame):
        self.frame = frame

    def stop(self):
        self.running = False


def get_local_ip():
    """è·å–æœ¬æœº IP"""
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except:
        return "192.168.132.166"


def main():
    parser = argparse.ArgumentParser(description='RK3576 YOLOv8 äººä½“è·Ÿè¸ª + MJPEG æ¨æµ')
    parser.add_argument('--source', type=str, default='36',
                       help='æ‘„åƒå¤´è®¾å¤‡å·')
    parser.add_argument('--model', type=str, default='yolov8.rknn',
                       help='RKNN æ¨¡å‹è·¯å¾„')
    parser.add_argument('--width', type=int, default=1280,
                       help='æ‘„åƒå¤´å®½åº¦')
    parser.add_argument('--height', type=int, default=720,
                       help='æ‘„åƒå¤´é«˜åº¦')
    parser.add_argument('--port', type=int, default=8080,
                       help='HTTP ç«¯å£')
    parser.add_argument('--stream-fps', type=int, default=30,
                       help='æ¨æµå¸§ç‡ï¼ˆé»˜è®¤30ï¼‰')
    parser.add_argument('--detect-fps', type=int, default=5,
                       help='æ£€æµ‹å¸§ç‡ï¼ˆé»˜è®¤5ï¼Œé€‚åˆç”µæœºè·Ÿè¸ªï¼‰')
    parser.add_argument('--show-all', action='store_true',
                       help='æ˜¾ç¤ºæ‰€æœ‰è·Ÿè¸ªç›®æ ‡ï¼ˆé»˜è®¤åªæ˜¾ç¤ºä¸»ç›®æ ‡ï¼‰')
    parser.add_argument('--output-coords', type=str, default='/tmp/tracker_coords.json',
                       help='è¾“å‡ºåæ ‡åˆ°æ–‡ä»¶ï¼ˆé»˜è®¤ï¼š/tmp/tracker_coords.jsonï¼‰')

    args = parser.parse_args()

    local_ip = get_local_ip()
    stream_url = f'http://{local_ip}:{args.port}/stream'

    print('='*60)
    print('RK3576 YOLOv8 äººä½“è·Ÿè¸ªç³»ç»Ÿ')
    print('='*60)
    print(f'æ‘„åƒå¤´: /dev/video{args.source}')
    print(f'åˆ†è¾¨ç‡: {args.width}x{args.height}')
    print(f'æ¨æµå¸§ç‡: {args.stream_fps} fps')
    print(f'æ£€æµ‹å¸§ç‡: {args.detect_fps} fps â­ é€‚åˆç”µæœºæ§åˆ¶')
    print(f'HTTP åœ°å€: {stream_url}')
    print(f'åæ ‡è¾“å‡º: {args.output_coords}')
    print('='*60)
    print('\nğŸ“º åœ¨æµè§ˆå™¨æˆ– PotPlayer ä¸­æŸ¥çœ‹:')
    print(f'   {stream_url}')
    print(f'\nğŸ“ ç”µæœºæ§åˆ¶ç¨‹åºå¯è¯»å–:')
    print(f'   {args.output_coords}\n')

    # åŠ è½½æ£€æµ‹å™¨
    try:
        detector = YOLOv8Detector(args.model)
    except RuntimeError as e:
        print(f'âŒ Error: {e}')
        return

    # åˆ›å»ºè·Ÿè¸ªå™¨
    tracker = PersonTracker(max_distance=100, iou_threshold=0.3)

    # æ‰“å¼€æ‘„åƒå¤´
    print(f'--> Opening camera /dev/video{args.source}...')
    cap = cv2.VideoCapture(int(args.source))

    if not cap.isOpened():
        print(f'âŒ Failed to open camera')
        detector.release()
        return

    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, args.width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, args.height)

    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    print(f'    Camera opened: {actual_w}x{actual_h}')
    tracker.set_image_size(actual_w, actual_h)

    # å¯åŠ¨ MJPEG æœåŠ¡å™¨
    print('--> å¯åŠ¨ HTTP MJPEG æœåŠ¡å™¨...')
    streamer = MJPEGStreamer(port=args.port)
    streamer.fps = args.stream_fps
    streamer.start()
    print(f'âœ… MJPEG æœåŠ¡å™¨å·²å¯åŠ¨')
    print('='*60)
    print('è·Ÿè¸ªè¿è¡Œä¸­... (æŒ‰ Ctrl+C åœæ­¢)')
    print('='*60)

    frame_count = 0
    detect_count = 0
    start_time = time.time()
    last_detect_time = time.time()
    detect_interval = 1.0 / args.detect_fps  # æ£€æµ‹é—´éš”

    last_coords = None

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            current_time = time.time()

            # æ£€æµ‹ï¼ˆæ§åˆ¶é¢‘ç‡ï¼‰
            should_detect = (current_time - last_detect_time) >= detect_interval

            if should_detect:
                detect_count += 1
                last_detect_time = current_time

                # æ‰§è¡Œæ£€æµ‹
                boxes, classes, scores = detector.detect_frame(frame)

                # æ›´æ–°è·Ÿè¸ªå™¨
                tracker.update(boxes, classes, scores)

                # è¾“å‡ºåæ ‡åˆ°æ–‡ä»¶ï¼ˆé»˜è®¤è‡ªåŠ¨ä¿å­˜ï¼‰
                if True:  # è‡ªåŠ¨ä¿å­˜åæ ‡
                    all_targets = tracker.get_all_targets()

                    # æ„å»ºåæ ‡æ•°æ®
                    coords_data = {
                        'system': {
                            'timestamp': time.time(),
                            'frame_count': frame_count,
                            'detect_count': detect_count,
                            'image': {
                                'width': actual_w,
                                'height': actual_h,
                                'center_x': actual_w / 2,
                                'center_y': actual_h / 2
                            }
                        },
                        'targets': []
                    }

                    # æ·»åŠ æ‰€æœ‰è·Ÿè¸ªç›®æ ‡
                    for target in all_targets:
                        coords_data['targets'].append(target.to_dict(actual_w, actual_h))

                    # ä¸»ç›®æ ‡ä¿¡æ¯
                    primary = tracker.get_primary_target()
                    if primary:
                        coords_data['primary_target'] = {
                            'id': primary.id,
                            'confidence': float(primary.score),
                            'position': {
                                'center': {
                                    'x': float(primary.smooth_x),
                                    'y': float(primary.smooth_y)
                                },
                                'relative': {
                                    'x': float(primary.rel_x),
                                    'y': float(primary.rel_y)
                                }
                            },
                            'motion': {
                                'velocity': {
                                    'vx': float(primary.vx),
                                    'vy': float(primary.vy),
                                    'speed': float(primary.speed)
                                },
                                'direction': {
                                    'angle': float(primary.angle),
                                    'text': primary.direction_str
                                }
                            }
                        }
                    else:
                        coords_data['primary_target'] = None

                    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                    coords_data['statistics'] = {
                        'total_targets': len(all_targets),
                        'avg_distance': float(np.mean([t.distance for t in all_targets])) if all_targets else 0,
                        'avg_speed': float(np.mean([t.speed for t in all_targets])) if all_targets else 0
                    }

                    try:
                        with open(args.output_coords, 'w') as f:
                            json.dump(coords_data, f, indent=2)
                    except Exception as e:
                        print(f'å†™å…¥åæ ‡æ–‡ä»¶å¤±è´¥: {e}')

            # ç»˜åˆ¶è·Ÿè¸ªç»“æœ
            frame = draw_tracking(frame, tracker, show_all=args.show_all)

            # æ·»åŠ ä¿¡æ¯å åŠ 
            primary = tracker.get_primary_target()
            info_lines = [
                f'Frame: {frame_count} | Detect: {detect_count}',
                f'Tracking: {len(tracker.get_all_targets())} person(s)',
            ]

            if primary:
                rel_x, rel_y = primary.get_relative_position()
                info_lines.append(f'Target {primary.id}: rel=({rel_x:.2f}, {rel_y:.2f})')

            # ç»˜åˆ¶ä¿¡æ¯
            y = 30
            for line in info_lines:
                cv2.putText(frame, line, (10, y),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                y += 25

            # æ¨æµ
            streamer.update_frame(frame)

            # æ‰“å°ç»Ÿè®¡
            if frame_count % 30 == 0:
                elapsed = time.time() - start_time
                stream_fps = frame_count / elapsed if elapsed > 0 else 0
                detect_fps = detect_count / elapsed if elapsed > 0 else 0
                print(f'[Frame {frame_count}] æ¨æµ: {stream_fps:.1f} fps | æ£€æµ‹: {detect_fps:.1f} fps | ç›®æ ‡: {len(tracker.get_all_targets())}')

    except KeyboardInterrupt:
        print('--> ç”¨æˆ·ä¸­æ–­')

    finally:
        print('='*60)
        print('è·Ÿè¸ªç»“æŸ')
        print(f'æ€»å¸§æ•°: {frame_count}')
        print(f'æ£€æµ‹æ¬¡æ•°: {detect_count}')

        elapsed = time.time() - start_time
        if elapsed > 0:
            stream_fps = frame_count / elapsed
            detect_fps = detect_count / elapsed
            print(f'å¹³å‡æ¨æµ FPS: {stream_fps:.2f}')
            print(f'å¹³å‡æ£€æµ‹ FPS: {detect_fps:.2f}')

        streamer.stop()
        cap.release()
        detector.release()

        print('='*60)


if __name__ == '__main__':
    main()
